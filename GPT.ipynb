{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT MODEL\n",
    "\n",
    "# MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, nheads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.nheads = nheads\n",
    "        self.head_dim = embed_size // nheads\n",
    "\n",
    "        self.fc_values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, x_queries, x_keys, x_values, mask):\n",
    "        batch_size = x_queries.shape[0] # x_keys.shape[0] x_values.shape[0]\n",
    "        seq_length = x_queries.shape[1] # x_keys.shape[1]\n",
    "        value_length = x_values.shape[1]\n",
    "        # (x_embedded) x_queries x_keys SHAPE: [batch_size, seq_length, embed_size]\n",
    "        # x_values SHAPE: [batch_size, value_len, embed_size]\n",
    "\n",
    "        # Split the embedding into nheads different pieces\n",
    "        x_queries = x_queries.reshape(batch_size, seq_length, self.nheads, self.head_dim)\n",
    "        x_keys = x_keys.reshape(batch_size, seq_length, self.nheads, self.head_dim)\n",
    "        x_values = x_values.reshape(batch_size, value_length, self.nheads, self.head_dim)\n",
    "        queries = self.fc_queries(x_queries)\n",
    "        keys = self.fc_keys(x_keys)\n",
    "        values = self.fc_values(x_values)\n",
    "        # queries keys values SHAPE: [batch_size, seq_length, nheads, head_dim]\n",
    "        # values SHAPE: [batch_size, value_length, nheads, head_dim]\n",
    "\n",
    "        # MatMul Queries Keys\n",
    "        similarity = torch.einsum(\"bqhd,bkhd->bhqk\", queries, keys)\n",
    "        if mask is not None:  # Add Mask\n",
    "            similarity += mask\n",
    "        similarity = torch.softmax(similarity / (self.head_dim ** 0.5), dim=3)\n",
    "        # similarity SHAPE: [batch_size, nheads, seq_length, seq_length]\n",
    "        \n",
    "        # MatMul Similarity Values\n",
    "        attention = torch.einsum(\"bhss,bvhd->bvhd\", similarity, values).reshape(batch_size, value_length, self.nheads * self.head_dim)\n",
    "\n",
    "        output_attention = self.fc_out(attention)\n",
    "        # output_attention attention SHAPE: [batch_size, value_length, embed_size (nheads * head_dim)]\n",
    "        return output_attention, attention\n",
    "\n",
    "# Feedforward\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, embed_size, dim_feedforward, dropout):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, dim_feedforward)\n",
    "        self.fc2 = nn.Linear(dim_feedforward, embed_size)        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output_feedforward = self.dropout(x)\n",
    "        return output_feedforward\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, nheads, dim_feedforward, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(embed_size, nheads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feedforward = Feedforward(embed_size, dim_feedforward, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # (x_embedded) x SHAPE: [batch_size, seq_length, embed_size]\n",
    "        output_attention, attention = self.multi_head_attention(x, x, x, attention_mask)\n",
    "        x_attention = self.norm1(x + output_attention)\n",
    "        # x output_attention SHAPE: [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        x_feedforward = self.feedforward(x_attention)\n",
    "        output_decoder = self.norm2(x_attention + x_feedforward)\n",
    "        # output_decoder feedforward SHAPE: [batch_size, seq_length, embed_size]\n",
    "        return output_decoder, attention\n",
    "\n",
    "# GPTModel\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocabulary, embed_size=8, nheads=4, dim_feedforward=128, decoder_layers=1, dropout=0.1, device='cpu', max_seq_length=500):\n",
    "        super(GPTModel, self).__init__()\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        assert embed_size % nheads == 0, \"Embedding size must be divisible by the number of heads\"\n",
    "        \n",
    "        self.device = device\n",
    "        self.vocabulary = vocabulary\n",
    "        self.vocab_size = len(vocabulary)\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embed_size).to(device)\n",
    "        self.pos_encoder = self.get_positional_encoding(max_seq_length)\n",
    "        self.decoder = Decoder(embed_size, nheads, dim_feedforward, dropout).to(device)\n",
    "        self.decoder_layers = nn.ModuleList([self.decoder for _ in range(decoder_layers)])\n",
    "        self.fc_out = nn.Linear(embed_size, self.vocab_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x SHAPE: [batch_size, seq_length]\n",
    "        seq_length = x.shape[1]\n",
    "        x_embedded = self.embedding(x)\n",
    "        # x_embedded SHAPE: [batch_size, seq_length, embed_size]\n",
    "        x_embedded_pos = x_embedded + self.pos_encoder[:, :seq_length]\n",
    "        # x_embedded_pos SHAPE: [batch_size, seq_length, embed_size]\n",
    "        attention_mask = self.get_self_attention_mask(seq_length)\n",
    "        # attention_mask SHAPE: [seq_length, seq_length]\n",
    "        output_decoder = x_embedded_pos\n",
    "        for decoder in self.decoder_layers:\n",
    "            output_decoder, attention = decoder(output_decoder, attention_mask)\n",
    "        # output_decoder SHAPE: [batch_size, seq_length, embed_size]\n",
    "        # attention SHAPE: [batch_size, seq_length, seq_length]\n",
    "        output = self.fc_out(output_decoder)\n",
    "        # output SHAPE: [batch_size, seq_length, vocab_size]\n",
    "        return output, attention\n",
    "\n",
    "    # Generate sinusoidal positional encodings\n",
    "    def get_positional_encoding(self, max_seq_length):\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.embed_size, 2) * -(math.log(10000.0) / self.embed_size))\n",
    "        pe = torch.zeros(max_seq_length, self.embed_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Sine for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Cosine for odd indices\n",
    "        return pe.unsqueeze(0).to(self.device)\n",
    "    \n",
    "    # Generate attention mask\n",
    "    def get_self_attention_mask(self, seq_length):\n",
    "        mask = (torch.triu(torch.ones(seq_length, seq_length)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask.to(self.device)\n",
    "\n",
    "\n",
    "# Training function \n",
    "def train_model(model, train_loader, val_loader, epochs=20, learning_rate=0.001, device='cpu'):\n",
    "    model.to(device)  # Move model to the correct device\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to the same device\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(inputs)\n",
    "            loss = criterion(output.view(-1, model.vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase (Set the model to evaluation mode)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                val_output, _ = model(val_inputs)\n",
    "                val_loss = criterion(val_output.view(-1, model.vocab_size), val_targets.view(-1))\n",
    "                total_val_loss += val_loss.item()\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        \n",
    "        # Print training and validation losses\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {round(avg_train_loss, 4)}, Val Loss: {round(avg_val_loss, 4)}')\n",
    "        #print(f'Epoch {epoch+1}, Train Loss: {round(avg_train_loss, 4)}')\n",
    "        scheduler.step()\n",
    "\n",
    "        # After validation completes, switch back to training mode\n",
    "        model.train()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love mac\n"
     ]
    }
   ],
   "source": [
    "# READ TXT\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        file_content = file.read()\n",
    "        file_content_with_newlines = file_content.replace('\\n', ' ZZZ ')\n",
    "    return file_content_with_newlines\n",
    "\n",
    "FILENAME = 'text.txt'\n",
    "text = read_file(FILENAME)\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length: 13\n",
      "{'like': 0, 'artifcial': 1, 'enjoy': 2, 'intelligence': 3, '.': 4, 'machine': 5, 'love': 6, 'artificial': 7, 'and': 8, 'about': 9, 'I': 10, 'learning': 11, 'care': 12}\n"
     ]
    }
   ],
   "source": [
    "# TOKENS AND VOCABULARY\n",
    "\n",
    "def tokenize_text(input_string):\n",
    "    tokens = input_string.split() # Simple split\n",
    "    #tokens = re.findall(r'\\w+|\\S+|\\s', input_string) # Split the string based on spaces and all non-word characters\n",
    "    #tokens = re.findall(r'\\w+|\\S', input_string) # Split the string based on all non-word characters\n",
    "    return tokens\n",
    "\n",
    "# Create tokens and vocabulary\n",
    "tokens = tokenize_text(text)\n",
    "vocabulary, inverse_vocab = {}, {}\n",
    "for i, word in enumerate(set(tokens), 0):\n",
    "    vocabulary[word] = i\n",
    "    inverse_vocab[i] = word\n",
    "\n",
    "print('Vocabulary Length:', len(vocabulary))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQUENCE_LENGTH: 4\n",
      "BATCH_SIZE: 1\n",
      "Number of BATCHs: 51\n",
      "Total Samples: 51\n",
      "\n",
      "Example:\n",
      "Input: ['intelligence', '.', 'I', 'love']\n",
      "Target: ['.', 'I', 'love', 'and']\n"
     ]
    }
   ],
   "source": [
    "# DATASETS AND DATALOADERS\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, vocabulary, sequence_length):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = [vocabulary[word] for word in tokens]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the input sequence of length `sequence_length` starting from `idx`\n",
    "        input_seq = torch.tensor(self.data[idx:idx + self.sequence_length], dtype=torch.long)\n",
    "        # The target sequence is offset by one token to predict the next token\n",
    "        target_seq = torch.tensor(self.data[idx + 1:idx + self.sequence_length + 1], dtype=torch.long)\n",
    "        return input_seq, target_seq\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH = 4\n",
    "BATCH_SIZE = 1\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "dataset = TextDataset(tokens, vocabulary, SEQUENCE_LENGTH)\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=VAL_RATIO, random_state=42, shuffle=True)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "\n",
    "print('SEQUENCE_LENGTH:', SEQUENCE_LENGTH)\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('Number of BATCHs:', len(train_loader))\n",
    "print('Total Samples:', len(train_loader)*BATCH_SIZE)\n",
    "\n",
    "print(\"\\nExample:\")\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(\"Input:\", [inverse_vocab[idx.item()] for idx in inputs[0]])\n",
    "    print(\"Target:\", [inverse_vocab[idx.item()] for idx in targets[0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "EMBED_SIZE = 512\n",
    "NHEADS = 16\n",
    "DIM_FEEDFORWARD = 1024\n",
    "DECODER_LAYERS = 1\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = GPTModel(vocabulary, embed_size=EMBED_SIZE, nheads=NHEADS, dim_feedforward=DIM_FEEDFORWARD, decoder_layers=DECODER_LAYERS, dropout=DROPOUT, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.8043, Val Loss: 0.8857\n",
      "Epoch 2, Train Loss: 0.8399, Val Loss: 0.9421\n",
      "Epoch 3, Train Loss: 0.7335, Val Loss: 1.2924\n",
      "Epoch 4, Train Loss: 0.7805, Val Loss: 0.7502\n",
      "Epoch 5, Train Loss: 0.7389, Val Loss: 0.9521\n",
      "Epoch 6, Train Loss: 0.7045, Val Loss: 0.7982\n",
      "Epoch 7, Train Loss: 0.6957, Val Loss: 0.712\n",
      "Epoch 8, Train Loss: 0.6367, Val Loss: 0.9864\n",
      "Epoch 9, Train Loss: 0.6561, Val Loss: 0.7738\n",
      "Epoch 10, Train Loss: 0.5801, Val Loss: 0.7649\n",
      "Epoch 11, Train Loss: 0.573, Val Loss: 0.8394\n",
      "Epoch 12, Train Loss: 0.5097, Val Loss: 0.6457\n",
      "Epoch 13, Train Loss: 0.5076, Val Loss: 0.8462\n",
      "Epoch 14, Train Loss: 0.4943, Val Loss: 0.975\n",
      "Epoch 15, Train Loss: 0.519, Val Loss: 0.672\n",
      "Epoch 16, Train Loss: 0.4077, Val Loss: 0.571\n",
      "Epoch 17, Train Loss: 0.4, Val Loss: 0.5275\n",
      "Epoch 18, Train Loss: 0.3933, Val Loss: 0.5202\n",
      "Epoch 19, Train Loss: 0.3669, Val Loss: 0.6279\n",
      "Epoch 20, Train Loss: 0.3628, Val Loss: 0.5133\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader, val_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and enjoy machine learning . I love and\n"
     ]
    }
   ],
   "source": [
    "# GENERATE TOKENS\n",
    "\n",
    "def generate_tokens(model, start_seq, generate_length=50, generate_context=10, temperature=0.00001):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Retrieve the vocabulary from the model\n",
    "    vocabulary = model.vocabulary \n",
    "    # Create inverse vocabulary to map indices back to words\n",
    "    inverse_vocab = {v: k for k, v in vocabulary.items()}\n",
    "    # Convert start sequence to tensor of indices\n",
    "    src = torch.tensor([vocabulary[word] for word in start_seq.split()], dtype=torch.long).unsqueeze(0).to(model.device)  # Now [1, seq_length]\n",
    "    # Iteratively generate new words\n",
    "    generated_seq = start_seq\n",
    "    for _ in range(generate_length):\n",
    "        with torch.no_grad():\n",
    "            # Predict from the model\n",
    "            output, attention = model(src)\n",
    "            # Apply temperature scaling to logits\n",
    "            logits = output[:, -1, :] / temperature\n",
    "            # Compute softmax probabilities and sample the next token value from the multinomial distribution\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            next_token_value = torch.multinomial(probabilities, 1).item()\n",
    "            # Convert token value back to token\n",
    "            next_token = inverse_vocab[next_token_value]\n",
    "            # Append generated token to the output sequence\n",
    "            if next_token == 'ZZZ':\n",
    "                generated_seq += '\\n'\n",
    "            else:\n",
    "                generated_seq += ' ' + next_token\n",
    "            #generated_seq += next_token\n",
    "            # Append to input for next prediction and slice to only account for context tokens\n",
    "            src = torch.cat([src, torch.tensor([[next_token_value]], dtype=torch.long).to(model.device)], dim=1)[:, -generate_context:]\n",
    "\n",
    "    return generated_seq\n",
    "\n",
    "\n",
    "START_SEQUENCE = \"I\"  # Initial sequence for text generation\n",
    "GENERATE_LENGTH = 100  # Number of tokens to generate\n",
    "GENERATE_CONTEXT = SEQUENCE_LENGTH  # Size of context used for generation\n",
    "TEMPERATURE = 0.001  # Temperature for controlling randomness in token generation\n",
    "generated_text = generate_tokens(model, start_seq=START_SEQUENCE, generate_length=GENERATE_LENGTH, generate_context=GENERATE_CONTEXT, temperature=TEMPERATURE)\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
